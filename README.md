AI 금융 로보어드바이저(트레이더)

1 모델 설명 						
   AI로 금융시장에서 시뮬레이션과 보상,예측 기반의 학습을 하여 최적매매결정을 도출 및 자동화 단계까지 진행하는 모델	
	
2. 개발 사유							
   1) 동적이고 데이터간의 상관성이 높은 환경에서 각종 시계열 예측 딥러닝 모델 LSTM CNN 등, 정상성을 가정하는 ARIMA 모델 및 LGBM 부스팅 방식들은 이와 같은 환경에서 오버피팅,편향 및 높은 분산을 보이는 등 낮은 성능을 보인다.							
   2) 이는 데이터 수를 늘리고 피쳐 수를 줄이는방식, Drop out , 데이터 노이즈 감소 방식 등으로 일부 해결이 가능하지만 여전히 lagging이 발생하고 분포가 달라지거나 휩소가 발생하면 예측값이 크게 벗어나는등 예측에 큰 리스크가 있다.	

   3) 또한 기존의 싱글에이전트 강화학습 방식도 하이퍼 파라미터에 매우 민감하여 실전에서 낮은 퍼포먼스를 보인다		
				
   4) 따라서 이러한 환경에서 적응시키고자 Asynchronous Method 사용 (각기 다른 환경과 policy를 학습하므로 연속데이터의 상관성 감소로 오버피팅 완화 , local minima 완화) 및 입력데이터 커스텀, 각종 분산감소화 샘플효율성 증가 방법을 통해 AI 예측 및 행동 최적화 프로젝트 진행한다.
  
          				
3. AI 개발 과정 도식		
   1] AI트레이더 개발 첫번째 과정 : 금융데이터 분석과 입력 지표개발
   ![image](https://github.com/user-attachments/assets/af1fb693-6959-40cb-9e7e-3e813d66ef4a)
   ![image](https://github.com/user-attachments/assets/fd79a78e-1ff1-4f2d-9d7d-b304583a637f)
   ![image](https://github.com/user-attachments/assets/ec1da69d-f7b5-4fd9-84b8-319a9c6f8ccb)
	
     1) 목적 : 주가의 디트랜드 로그수익률 데이터가 정규분포에 해당하는지, 이상치가 많은 동적인 분포인지 확인하기 위함.							
     2) 사용 용도 : 해당 분석 결과를 참고하여 어떤 입력데이터를 개발할지, 이상치에 어떻게 대처할지, 어떤 신경망의 모델과 어떤 강화학습 방법을 사용할지 계획 수립	
     3) 단계 : 해당 분석 실행후 "2-2)입력데이터(지표)" 와 같이 입력데이터 지표를 개발  
     4) 결과 : 주가는 Fat tail risk 분포를 가지며(이상치가 정규분포보다 많은 경우) 이는 기술적 변동성에 의한 휩소, 이슈에의한 변동에 의해 발생				
     5) 전략 : 따라서 오버피팅을 최소화시키기 위해 딥러닝만으로 개발하기보단 멀티Agent를 가지는 강화학습 알고리즘으로 동적인분포에서 안정성을 높이고 역추세,추세,시장이슈데이터를 입력으로 활용한다															

   2] 알고리즘 및 프로세스							
     1) 가정 : 1번의 분석 결과(Fat tail 분포)를 가정하여 개발 시작			
     2) 입력데이터(지표) : Fat tail 분포는 기술적 변동성(세력에의한 매도,매수)과 
   이슈에의한 휩소에서 발생하므로 역추세, 추세, 시장이슈데이터(정량화, 선택사항)를 포함
     3) 방안 : 위 지표를 참고하여 예측이 필요한 환경에서 다수 에이전트가 비동기적으로            수요 또는 가격등을 학습하고 예측값을 도출하며 최적의사결정까지 진행한다


   ![image](https://github.com/user-attachments/assets/efcb1189-6155-42d7-a5af-286529decf2f)
   ![image](https://github.com/user-attachments/assets/e635269d-3dd3-4b6f-ab77-f2dbe9d538f2)
   ![image](https://github.com/user-attachments/assets/6774446b-484b-4381-ab8a-418e2ab082ad)

설명
   1] 비평/예측 모델 Critic 뉴럴네트워크와 행동네트워크 Actor 뉴럴네트워크가 강화학습을 통해 학습한다
   2] actor network는 action을 선택하고 critic network는 Q(s,a) 를 평가 한다. 이는 학습 분산을 줄여준다
   3] 크리틱 네트워크로 GAE를 추정하므로 얼마나 좋은 행동가치인지 뿐만아니라 얼마나 더 좋아질수 있는지도 고려한다
   4] 여러 데이터를 받으면 Actor 와 critic 신경망이 이를 받아들이는데 처음에 여기서 나온 정책과 데이터기반의 가치를 계산
   5] 여기서 타겟값과 예측값을 구하게되는데 타겟값은 리워드 + 감마 곱하기 예측값으로 해서 넥스트 스탭의 미래 행동가치를 AI가 추정해서 타겟으로 설정
   6]이 타겟값에 현재의 상태가치를 빼면 델타가 되는데 이는 Q-V꼴의 어드밴티지가 됩니다
   7] 이 어드밴티지를 GAE로 확장해서 구한것 = 델타 + 감마람다*델타_t+1 ..... 로 미래 어드밴티지 추정값까지 고려
   8] 입실론과 r(현 정책/과거정책)으로 적절한 범위 신뢰구간을 설정하고 목적함수를 구하여 critic과 actor에 역전파

   
4. 백테스트, 전진분석 , 실전 투입 및 개선	
  ![image](https://github.com/user-attachments/assets/7c03cc7a-cf39-4c25-a4db-251522728498)

  
5. 오버피팅 문제 완화 노력								
	1) Raw 데이터를 예측 논리에 맞게 잘 커스텀하여 데이터의 노이즈를 줄이고 repaint 현상을 막는다.  (ex. 예측시 분포에서 잘나타나지 않는 특이 변수를 설명하기위한 입력데이터 가공하는 등)									
	2) 데이터 차원이 늘어날수록 차원의 저주를 피하기 위해 주성분 분석을 통해 상관계수가 높은 데이터는 제거하여 사용							
	3) 리플레이 버퍼를 사용함으로써 샘플효율성을 증대시킨다				
	4) new policy가 old policy 와 크게 다르지않도록 Clipping 하여 안정성이 높인다	
	5) GAE Advantage를 사용하여 Advantage를 잘추산한다. 이로인해 분산을 더 적절하게 감소										
	6) 신뢰 지역(Trust region) 에서 GAE를 구하고 r세타를 연산하는 덕에 buffer를 사용할수 있고 next batch에서 좋지않은 policy를 뽑을 경우 재사용하지 않는다			
	7) 새로운 정책이 기존 정책에서 너무 멀리 바뀌는 것을 피하기 위해 대리 목표를 활용하여 min을 취함으로 샘플 효율성 문제를 해결한다.					
	8) 정책 업데이트를 정규화하고 교육 데이터를 재사용할 수 있기 때문에 대리 목표는 핵심 기능이다. 따라서 on policy 알고리즘 이지만 on policy 의 수렴성과 대리목표(Surrogate loss) 사용으로 off policy의 장점인 샘플 효율성을 가지게 한다																																		
																					 
6. 겪었던 문제와 개선					
1) critic loss 및 actor loss 가 수렴하지 않는 현상					
	방안 1: 하이퍼 파라미터 설정의 문제 확인						
	방안 2: 뉴럴넷의 복잡성 확인							
	방안 3: 보상함수 설정 확인							
	방안 4 :가중치 소실 확인 							
	방안 5 : 연산중 inf 또는 None값 확인/ device 설정문제 확인
	
2) 보상함수를 전체 가치로 설정하지 않고 일부 예측의 가치로 설정하여 예측 성과가 떨어지는 문제								
	방안1: 이를 전체 가치를 활용하는 보상함수로 재설정하여 문제를 해결

3) 비동기 학습 및 리플레이버퍼, 그래디언트 클리핑, 분산감소 방법론등을 사용했지만 동적인 환경에서 여전히 오버피팅의 위험이 존재			
	방안1: Raw 데이터를 전처리하여 그대로 넣지않고 새롭게 가공하여 노이즈 감소 ,추세, 변동성 포함된 입력 지표데이터 사용													
4) 10분 간격의 데이터를 학습하고 Test set 또는 실시간 데이터를 읽을때 시작 시점에 따라 성능 차이 발생							
	방안1 : 실제 학습했던 데이터 텀과 유사하게 읽어들이기 위해 학습시 마지막 기간값을 토대로 시간을 계산하여 실시간 데이터를 읽어들임

